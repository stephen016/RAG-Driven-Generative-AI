{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f903c5-84b3-4703-855b-1dab4ddfb6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai==1.40.3\n",
    "#!pip install --upgrade openai\n",
    "#!pip install scikit-learn\n",
    "#!pip install spacy\n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7929ab8f-6bed-46d3-926d-6b8bf9556d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f95555-21a4-49d2-8c79-bce993b8bd52",
   "metadata": {},
   "source": [
    "### generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21cbe0b9-35c0-4eba-8297-65f71cb5ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key_path=\"../../openai-key.txt\"\n",
    "f = open(openai_key_path, \"r\")\n",
    "API_KEY=f.readline().strip()\n",
    "f.close()\n",
    "os.environ['OPENAI_API_KEY'] =API_KEY\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fde1a4cf-d013-44f1-ac2a-3bcfc83ddb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "gptmodel=\"gpt-4o-mini\"\n",
    "def call_llm_with_full_text(itext):\n",
    "    # Join all lines to form a single string\n",
    "    text_input = '\\n'.join(itext)\n",
    "    prompt = f\"Please elaborate on the following content:\\n{text_input}\"\n",
    "\n",
    "    try:\n",
    "      response = client.chat.completions.create(\n",
    "         model=gptmodel,\n",
    "         messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert Natural Language Processing exercise expert.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"1.You can explain read the input and answer in detail\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "         ],\n",
    "         temperature=0.1  # Add the temperature parameter here and other parameters you need\n",
    "        )\n",
    "      return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return str(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61d40d47-2ed3-4136-b897-7efe4a3db6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_formatted_response(response):\n",
    "    # Define the width for wrapping the text\n",
    "    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed\n",
    "    wrapped_text = wrapper.fill(text=response)\n",
    "\n",
    "    # Print the formatted response with a header and footer\n",
    "    print(\"Response:\")\n",
    "    print(\"---------------\")\n",
    "    print(wrapped_text)\n",
    "    print(\"---------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866fc7ae-57fb-4eb6-815b-7f3e1513b46e",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af056016-64b3-45dc-aef9-ff9ec79bc8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_records = [\n",
    "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
    "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
    "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
    "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
    "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
    "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
    "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
    "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
    "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
    "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
    "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
    "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
    "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
    "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
    "    \"The retrieved documents are then fed into the language model.\",\n",
    "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
    "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
    "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
    "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
    "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
    "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
    "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
    "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
    "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
    "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
    "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
    "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\",\n",
    "    \"A RAG vector store is a database or dataset that contains vectorized data points.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1817100a-278a-4485-a65a-afbd616357f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
      "in the field of artificial intelligence, particularly within the realm of\n",
      "natural language processing (NLP). It innovatively combines the capabilities of\n",
      "neural network-based language models with retrieval systems to enhance the\n",
      "generation of text, making it more accurate, informative, and contextually\n",
      "relevant. This methodology leverages the strengths of both generative and\n",
      "retrieval architectures to tackle complex tasks that require not only linguistic\n",
      "fluency but also factual correctness and depth of knowledge. At the core of\n",
      "Retrieval Augmented Generation (RAG) is a generative model, typically a\n",
      "transformer-based neural network, similar to those used in models like GPT\n",
      "(Generative Pre-trained Transformer) or BERT (Bidirectional Encoder\n",
      "Representations from Transformers). This component is responsible for producing\n",
      "coherent and contextually appropriate language outputs based on a mixture of\n",
      "input prompts and additional information fetched by the retrieval component.\n",
      "Complementing the language model is the retrieval system, which is usually built\n",
      "on a database of documents or a corpus of texts. This system uses techniques\n",
      "from information retrieval to find and fetch documents that are relevant to the\n",
      "input query or prompt. The mechanism of relevance determination can range from\n",
      "simple keyword matching to more complex semantic search algorithms which\n",
      "interpret the meaning behind the query to find the best matches. This component\n",
      "merges the outputs from the language model and the retrieval system. It\n",
      "effectively synthesizes the raw data fetched by the retrieval system into the\n",
      "generative process of the language model. The integrator ensures that the\n",
      "information from the retrieval system is seamlessly incorporated into the final\n",
      "text output, enhancing the model's ability to generate responses that are not\n",
      "only fluent and grammatically correct but also rich in factual details and\n",
      "context-specific nuances. When a query or prompt is received, the system first\n",
      "processes it to understand the requirement or the context. Based on the\n",
      "processed query, the retrieval system searches through its database to find\n",
      "relevant documents or information snippets. This retrieval is guided by the\n",
      "similarity of content in the documents to the query, which can be determined\n",
      "through various techniques like vector embeddings or semantic similarity\n",
      "measures. The retrieved documents are then fed into the language model. In some\n",
      "implementations, this integration happens at the token level, where the model\n",
      "can access and incorporate specific pieces of information from the retrieved\n",
      "texts dynamically as it generates each part of the response. The language model,\n",
      "now augmented with direct access to retrieved information, generates a response.\n",
      "This response is not only influenced by the training of the model but also by\n",
      "the specific facts and details contained in the retrieved documents, making it\n",
      "more tailored and accurate. By directly incorporating information from external\n",
      "sources, Retrieval Augmented Generation (RAG) models can produce responses that\n",
      "are more factual and relevant to the given query. This is particularly useful in\n",
      "domains like medical advice, technical support, and other areas where precision\n",
      "and up-to-date knowledge are crucial. Retrieval Augmented Generation (RAG)\n",
      "systems can dynamically adapt to new information since they retrieve data in\n",
      "real-time from their databases. This allows them to remain current with the\n",
      "latest knowledge and trends without needing frequent retraining. With access to\n",
      "a wide range of documents, Retrieval Augmented Generation (RAG) systems can\n",
      "provide detailed and nuanced answers that a standalone language model might not\n",
      "be capable of generating based solely on its pre-trained knowledge. While\n",
      "Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes\n",
      "with its challenges. These include the complexity of integrating retrieval and\n",
      "generation systems, the computational overhead associated with real-time data\n",
      "retrieval, and the need for maintaining a large, up-to-date, and high-quality\n",
      "database of retrievable texts. Furthermore, ensuring the relevance and accuracy\n",
      "of the retrieved information remains a significant challenge, as does managing\n",
      "the potential for introducing biases or errors from the external sources. In\n",
      "summary, Retrieval Augmented Generation represents a significant advancement in\n",
      "the field of artificial intelligence, merging the best of retrieval-based and\n",
      "generative technologies to create systems that not only understand and generate\n",
      "natural language but also deeply comprehend and utilize the vast amounts of\n",
      "information available in textual form. A RAG vector store is a database or\n",
      "dataset that contains vectorized data points.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "paragraph = ' '.join(db_records)\n",
    "wrapped_text = textwrap.fill(paragraph, width=80)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08bd60e-b1ab-4541-aacd-e7d8282a61db",
   "metadata": {},
   "source": [
    "### query without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f870b2f1-bbe0-4aa8-aae6-5d0fd0f06a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"define a rag store\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "089e0970-a9ce-4028-8ba0-d1feffb35f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "It seems like you've provided a sequence of letters that spell out \"define a rag\n",
      "store\" when combined. Let's break this down into its components and elaborate on\n",
      "the concept of a \"rag store.\"  ### Definition of a Rag Store  A **rag store** is\n",
      "a type of retail establishment that specializes in selling second-hand clothing,\n",
      "often referred to as \"rags.\" These stores typically focus on providing\n",
      "affordable clothing options, and they may sell a variety of items, including:\n",
      "1. **Used Clothing**: Garments that have been previously owned and are resold at\n",
      "a lower price. This can include anything from casual wear to formal attire.  2.\n",
      "**Vintage Items**: Some rag stores may specialize in vintage clothing, which\n",
      "refers to garments that are at least 20 years old and often have a unique style\n",
      "that reflects the fashion of a particular era.  3. **Textiles and Fabrics**: In\n",
      "addition to clothing, rag stores may also sell various types of fabric remnants,\n",
      "which can be used for sewing or crafting projects.  4. **Accessories**: Many rag\n",
      "stores also carry accessories such as bags, hats, belts, and jewelry that\n",
      "complement the clothing they sell.  ### Purpose and Benefits of Rag Stores  -\n",
      "**Affordability**: Rag stores provide an economical option for consumers looking\n",
      "for clothing without the high price tag associated with new items.  -\n",
      "**Sustainability**: By promoting the reuse of clothing, rag stores contribute to\n",
      "environmental sustainability. They help reduce waste by giving garments a second\n",
      "life and minimizing the demand for new clothing production.  - **Unique Finds**:\n",
      "Shoppers often enjoy the thrill of hunting for unique or one-of-a-kind items\n",
      "that may not be available in mainstream retail stores.  - **Support for\n",
      "Charities**: Some rag stores are operated by non-profit organizations, and\n",
      "proceeds from sales may go towards charitable causes, making shopping at these\n",
      "stores a way to support community initiatives.  ### Conclusion  In summary, a\n",
      "rag store is a retail outlet that focuses on selling second-hand clothing and\n",
      "related items. These stores play an important role in promoting sustainable\n",
      "fashion practices, providing affordable clothing options, and offering unique\n",
      "shopping experiences. Whether for budget-conscious consumers or those seeking\n",
      "vintage treasures, rag stores serve a diverse clientele and contribute\n",
      "positively to the fashion ecosystem.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(query)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb965c46-d170-4eb8-9d66-7180d2bc6a89",
   "metadata": {},
   "source": [
    "## RAG and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9fefd4-d465-4029-a965-ec9f7282ca84",
   "metadata": {},
   "source": [
    "### Retrieval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e5d6c17-e3bb-4f66-b1f3-0c0369a29e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59fd59ff-c5f7-43f4-a670-992dc9850e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity with Tfid\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        use_idf=True,\n",
    "        norm='l2',\n",
    "        ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "        sublinear_tf=True,   # Apply sublinear TF scaling\n",
    "        analyzer='word'      # You could also experiment with 'char' or 'char_wb' for character-level features\n",
    "    )\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
    "    return similarity[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd881131-1013-4d9b-9cb9-2e93a7dbcbcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# enhanced similarity\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      4\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# enhanced similarity\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    lemmatized_words = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        lemmatized_words.append(token.lemma_)\n",
    "    return lemmatized_words\n",
    "\n",
    "def expand_with_synonyms(words):\n",
    "    expanded_words = words.copy()\n",
    "    for word in words:\n",
    "        expanded_words.extend(get_synonyms(word))\n",
    "    return expanded_words\n",
    "\n",
    "def calculate_enhanced_similarity(text1, text2):\n",
    "    # Preprocess and tokenize texts\n",
    "    words1 = preprocess_text(text1)\n",
    "    words2 = preprocess_text(text2)\n",
    "\n",
    "    # Expand with synonyms\n",
    "    words1_expanded = expand_with_synonyms(words1)\n",
    "    words2_expanded = expand_with_synonyms(words2)\n",
    "\n",
    "    # Count word frequencies\n",
    "    freq1 = Counter(words1_expanded)\n",
    "    freq2 = Counter(words2_expanded)\n",
    "\n",
    "    # Create a set of all unique words\n",
    "    unique_words = set(freq1.keys()).union(set(freq2.keys()))\n",
    "\n",
    "    # Create frequency vectors\n",
    "    vector1 = [freq1[word] for word in unique_words]\n",
    "    vector2 = [freq2[word] for word in unique_words]\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    vector1 = np.array(vector1)\n",
    "    vector2 = np.array(vector2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f182941a-6697-4800-a0bc-e3e337dbdcf1",
   "metadata": {},
   "source": [
    "### Naive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6fb00a4-4e0f-4311-bd49-d4351014163d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Keyword Score: 3\n",
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_best_match_keyword_search(query, db_records):\n",
    "    best_score = 0\n",
    "    best_record = None\n",
    "\n",
    "    # Split the query into individual keywords\n",
    "    query_keywords = set(query.lower().split())\n",
    "\n",
    "    # Iterate through each record in db_records\n",
    "    for record in db_records:\n",
    "        # Split the record into keywords\n",
    "        record_keywords = set(record.lower().split())\n",
    "\n",
    "        # Calculate the number of common keywords\n",
    "        common_keywords = query_keywords.intersection(record_keywords)\n",
    "        current_score = len(common_keywords)\n",
    "\n",
    "        # Update the best score and record if the current score is higher\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_record = record\n",
    "\n",
    "    return best_score, best_record\n",
    "\n",
    "# Assuming 'query' and 'db_records' are defined in previous cells in your Colab notebook\n",
    "best_keyword_score, best_matching_record = find_best_match_keyword_search(query, db_records)\n",
    "\n",
    "print(f\"Best Keyword Score: {best_keyword_score}\")\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5d672b5-dcf5-4a31-af13-188e85810cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.126\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "score = calculate_cosine_similarity(query, best_matching_record)\n",
    "print(f\"Best Cosine Similarity Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553d9aa-f495-4f74-98f7-67e2f53ac933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, response)\n",
    "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca652a8-9388-4d13-954c-fad1a5aeedb9",
   "metadata": {},
   "source": [
    "### generation with augmented input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fa56669-15d0-43ec-8d9b-828d578173e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "A **RAG vector store** refers to a specialized database or dataset designed to\n",
      "hold vectorized data points, particularly in the context of Retrieval-Augmented\n",
      "Generation (RAG) models in Natural Language Processing (NLP). Let's break down\n",
      "the concept further:  ### What is a Vector Store?  1. **Vector Representation**:\n",
      "In machine learning and NLP, data points (such as words, sentences, or\n",
      "documents) are often represented as vectors. A vector is a mathematical\n",
      "representation that captures the semantic meaning of the data in a multi-\n",
      "dimensional space. For instance, words with similar meanings will have vectors\n",
      "that are close to each other in this space.  2. **Storage of Vectors**: A vector\n",
      "store is essentially a repository where these vector representations are stored.\n",
      "This allows for efficient retrieval and manipulation of the data points based on\n",
      "their vector representations.  ### What is RAG?  1. **Retrieval-Augmented\n",
      "Generation (RAG)**: RAG is a model architecture that combines retrieval and\n",
      "generation tasks. It retrieves relevant information from a dataset (often stored\n",
      "in a vector store) and uses that information to generate coherent and\n",
      "contextually relevant text. This approach enhances the model's ability to\n",
      "provide accurate and informative responses by grounding its outputs in real\n",
      "data.  ### Purpose of a RAG Vector Store  1. **Efficient Retrieval**: The\n",
      "primary purpose of a RAG vector store is to facilitate the quick retrieval of\n",
      "relevant data points based on a query. When a user inputs a question or prompt,\n",
      "the model can quickly search the vector store for the most relevant vectors\n",
      "(data points) that can inform its response.  2. **Scalability**: Vector stores\n",
      "are designed to handle large volumes of data efficiently. As the amount of data\n",
      "grows, the vector store can scale to accommodate new data points without\n",
      "significant degradation in performance.  3. **Semantic Search**: By using vector\n",
      "representations, a RAG vector store enables semantic search capabilities. This\n",
      "means that the retrieval process can find relevant information based on meaning\n",
      "rather than just keyword matching, leading to more accurate and contextually\n",
      "appropriate results.  ### Applications  1. **Question Answering**: In\n",
      "applications where users ask questions, a RAG vector store can retrieve relevant\n",
      "documents or snippets that contain the answers, which the model can then use to\n",
      "generate a response.  2. **Chatbots and Virtual Assistants**: RAG vector stores\n",
      "can enhance the performance of chatbots by allowing them to pull in relevant\n",
      "information from a vast dataset, improving the quality of interactions.  3.\n",
      "**Content Generation**: For content creation tasks, a RAG vector store can\n",
      "provide the necessary background information or context that the model can use\n",
      "to generate informative and relevant text.  ### Conclusion  In summary, a RAG\n",
      "vector store is a crucial component in the architecture of Retrieval-Augmented\n",
      "Generation models, enabling efficient storage, retrieval, and utilization of\n",
      "vectorized data points. This enhances the model's ability to generate accurate\n",
      "and contextually relevant outputs, making it a valuable tool in various NLP\n",
      "applications.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_input=query+ \": \"+ best_matching_record\n",
    "llm_response = call_llm_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9189e209-9d10-4449-96a3-c08b5c36042f",
   "metadata": {},
   "source": [
    "### Advanced RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06627e25-334b-42ca-903d-0d831ca41b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector search\n",
    "\n",
    "def find_best_match(text_input, records):\n",
    "    best_score = 0\n",
    "    best_record = None\n",
    "    for record in records:\n",
    "        current_score = calculate_cosine_similarity(text_input, record)\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_record = record\n",
    "    return best_score, best_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6da5f5ea-a7e5-42e7-8e43-eaad023d032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_similarity_score, best_matching_record = find_best_match(query, db_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c780cd02-9f3f-41ae-ab61-9f81e1124cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.407\n",
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# index-based search\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def setup_vectorizer(records):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(records)\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "def find_best_match(query, vectorizer, tfidf_matrix):\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
    "    best_index = similarities.argmax()  # Get the index of the highest similarity score\n",
    "    best_score = similarities[0, best_index]\n",
    "    return best_score, best_index\n",
    "\n",
    "vectorizer, tfidf_matrix = setup_vectorizer(db_records)\n",
    "\n",
    "best_similarity_score, best_index = find_best_match(query, vectorizer, tfidf_matrix)\n",
    "best_matching_record = db_records[best_index]\n",
    "\n",
    "# Cosine Similarity\n",
    "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da48f7df-11e7-406d-a235-e53c90130a7e",
   "metadata": {},
   "source": [
    "### Modular RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2f2cc4e3-aaf0-42cf-afcd-27f782121d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modular RAG can combine different approaches\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class RetrievalComponent:\n",
    "    def __init__(self, method='vector'):\n",
    "        self.method = method\n",
    "        if self.method == 'vector' or self.method == 'indexed':\n",
    "            self.vectorizer = TfidfVectorizer()\n",
    "            self.tfidf_matrix = None\n",
    "\n",
    "    def fit(self, records):\n",
    "      self.documents = records  # Initialize self.documents here\n",
    "      if self.method == 'vector' or self.method == 'indexed':\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(records)\n",
    "\n",
    "    def retrieve(self, query):\n",
    "        if self.method == 'keyword':\n",
    "            return self.keyword_search(query)\n",
    "        elif self.method == 'vector':\n",
    "            return self.vector_search(query)\n",
    "        elif self.method == 'indexed':\n",
    "            return self.indexed_search(query)\n",
    "\n",
    "    def keyword_search(self, query):\n",
    "        best_score = 0\n",
    "        best_record = None\n",
    "        query_keywords = set(query.lower().split())\n",
    "        for index, doc in enumerate(self.documents):\n",
    "            doc_keywords = set(doc.lower().split())\n",
    "            common_keywords = query_keywords.intersection(doc_keywords)\n",
    "            score = len(common_keywords)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_record = self.documents[index]\n",
    "        return best_record\n",
    "\n",
    "    def vector_search(self, query):\n",
    "        query_tfidf = self.vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
    "        best_index = similarities.argmax()\n",
    "        return db_records[best_index]\n",
    "\n",
    "    def indexed_search(self, query):\n",
    "        # Assuming the tfidf_matrix is precomputed and stored\n",
    "        query_tfidf = self.vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
    "        best_index = similarities.argmax()\n",
    "        return db_records[best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c6d396bb-6efb-4f3a-8837-1013280e8467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "retrieval = RetrievalComponent(method='vector')  # Choose from 'keyword', 'vector', 'indexed'\n",
    "retrieval.fit(db_records)\n",
    "best_matching_record = retrieval.retrieve(query)\n",
    "\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a4046f-8881-4ef2-bb3f-2ef223b97ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
